# Transformer 架构完整实现指南

这个项目提供了从零开始的 Transformer 模型完整实现，包括所有核心组件、可视化和实战训练示例。

## 项目结构

```
Transformers/
├── transformers/
│   ├── Transformer_Implementation.ipynb  # 主要实现笔记本
│   └── training_curve.png                # 训练曲线图
└── Jupyters/
    └── transformer_attention_mechanisms.ipynb
```

## 笔记本内容

### Transformer_Implementation.ipynb

#### 第1部分：ScaledDotProductAttention
- 缩放点积注意力的核心实现
- 公式：Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
- 支持掩码功能

#### 第2部分：MultiHeadAttention  
- 多头注意力机制
- 使用 8 个平行头增加表达能力
- 包含线性投影层

#### 第3部分：FeedForwardNetwork
- 位置级别的前馈网络
- 两层线性层，中间使用 GELU 激活
- 维度扩展 4 倍后恢复

#### 第4部分：PositionalEncoding
- 使用三角函数的位置编码
- 支持序列长度外推
- 相对位置通过向量差表现

#### 第5部分：EncoderLayer 和 DecoderLayer
- EncoderLayer: 自注意力 → 前馈网络
- DecoderLayer: 自注意力 → 交叉注意力 → 前馈网络
- 所有层都包含层归一化和残差连接

#### 第6部分：完整 Transformer 模型
- 编码器和解码器堆栈
- 词嵌入和位置编码
- 输出投影层

#### 第7部分：实战示例
- **任务**: 序列复制任务
- **数据集**: 100 个样本
- **训练**: 20 个 epoch
- **结果**: 100% 准确率 ✓

#### 第8部分：总结与设计决策
- 详细解释每个组件的设计原因
- 复杂度分析
- 与 RNN 的对比

## 运行结果

### 模型参数
```
总参数数: 2,652,180
- 词嵌入: 10,240 (0.39%)
- 编码器: 1,318,912 (49.67%)
- 解码器: 1,318,912 (49.67%)
- 输出投影: 4,096 (0.15%)
```

### 训练曲线
- Epoch 1: Loss = 1.6312
- Epoch 5: Loss = 0.0031  
- Epoch 10: Loss = 0.0015
- Epoch 15: Loss = 0.0011
- Epoch 20: Loss = 0.0008

### 测试结果
```
样例 1: 准确率 100%
样例 2: 准确率 100%
样例 3: 准确率 100%
样例 4: 准确率 100%
总体准确率: 100.0% ✓
```

## 关键设计决策

### 1. 为什么是自注意力而不是 RNN?
✅ **并行性**: 注意力可以并行计算，RNN 必须顺序处理  
✅ **长距离依赖**: 直接连接所有位置，梯度流通更好  
✅ **灵活性**: 可以学习任意的位置关系  

### 2. 为什么需要多头注意力?
- 不同的头可以关注不同的模式
- 增加表达能力，类似 CNN 的多个过滤器
- 并行计算代价很小

### 3. 为什么需要位置编码?
- 自注意力本身没有位置信息
- 三角函数编码可以外推到更长序列
- 模型可以学习相对距离的重要性

### 4. 为什么需要层归一化和残差连接?
- 层归一化稳定训练，不依赖 batch size
- 残差连接解决梯度消失问题

### 5. 为什么用因果掩码?
- 防止解码器看到未来词汇
- 训练和推理行为一致

## 复杂度分析

| 配置 | 注意力复杂度 | 前馈复杂度 | 总复杂度 |
|-----|-----------|---------|--------|
| seq_len=128 | 0.05G | 0.20G | 0.25G |
| seq_len=256 | 0.20G | 0.40G | 0.60G |
| seq_len=512 | 0.81G | 0.81G | 1.61G |
| seq_len=1024 | 3.22G | 1.61G | 4.83G |
| seq_len=2048 | 12.88G | 3.22G | 16.11G |

虽然注意力是 O(n²)，但 Transformer 可以充分利用 GPU 并行计算，实际上比 RNN 快很多。

## 模型架构流程

```
输入 ID 序列
    ↓
词嵌入 + 位置编码
    ↓
编码器 (N层)
├─ 多头注意力
├─ 前馈网络
└─ 层归一化和残差连接
    ↓
编码器输出
    ↓
目标序列 ID（左移）
    ↓
词嵌入 + 位置编码
    ↓
解码器 (N层)
├─ 自注意力（因果）
├─ 交叉注意力（编码器输出）
├─ 前馈网络
└─ 层归一化和残差连接
    ↓
解码器输出
    ↓
线性投影 + Softmax
    ↓
概率分布
```

## 生成的文件

- `training_curve.png`: 训练过程中的损失曲线
- `Transformer_Implementation.ipynb`: 完整的实现笔记本

## 推荐阅读

1. "Attention Is All You Need" (Vaswani et al., 2017)
2. BERT、GPT 等相关论文
3. Hugging Face Transformers 库

## 许可证

教学用途

## 联系方式

关于这个实现的任何问题，请参考论文或 Hugging Face 文档。
